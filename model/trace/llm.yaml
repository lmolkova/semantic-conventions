groups:
  - id: gen_ai.llm.request
    type: span
    brief: >
      A request to an LLM is modeled as a span in a trace. The span name should be a low cardinality value representing the request made to an LLM, like the name of the API endpoint being called.
    attributes:
      - ref: gen_ai.llm.system
        requirement_level: recommended
        note: >
          The name of the LLM foundation model vendor, if applicable. If not using a vendor-supplied model, this field is left blank.
      - ref: gen_ai.llm.request.model
        requirement_level: required
        note: >
            The name of the LLM a request is being made to. If the LLM is supplied by a vendor,
            then the value must be the exact name of the model requested. If the LLM is a fine-tuned
            custom model, the value should have a more specific name than the base model that's been fine-tuned.
      - ref: gen_ai.llm.request.max_tokens
        requirement_level: recommended
      - ref: gen_ai.llm.request.temperature
        requirement_level: recommended
      - ref: gen_ai.llm.request.top_p
        requirement_level: recommended
      - ref: gen_ai.llm.response.id
        requirement_level: recommended
      - ref: gen_ai.llm.response.model
        requirement_level: required
        note: >
          The name of the LLM a response is being made to. If the LLM is supplied by a vendor,
          then the value must be the exact name of the model actually used. If the LLM is a
          fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned.
      - ref: gen_ai.llm.response.finish_reason
        requirement_level: recommended
      - ref: gen_ai.llm.usage.prompt_tokens
        requirement_level: recommended
      - ref: gen_ai.llm.usage.completion_tokens
        requirement_level: recommended
    events:
      - gen_ai.llm.content.prompt
      - gen_ai.llm.content.completion

  - id: gen_ai.llm.content.prompt
    name: gen_ai.llm.content.prompt
    type: event
    brief: >
      In the lifetime of an LLM span, events for prompts sent and completions received
      may be created, depending on the configuration of the instrumentation.
    attributes:
      - ref: gen_ai.llm.prompt
        requirement_level: recommended
        note: >
          The full prompt sent to an LLM in a request, structured as a JSON in OpenAI's format.

  - id: gen_ai.llm.content.completion
    name: gen_ai.llm.content.completion
    type: event
    brief: >
      In the lifetime of an LLM span, events for prompts sent and completions received
      may be created, depending on the configuration of the instrumentation.
    attributes:
      - ref: gen_ai.llm.completion
        requirement_level: recommended
        note: >
          The full response from an LLM, structured as a JSON in OpenAI's format.
